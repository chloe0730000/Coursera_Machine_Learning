{\rtf1\ansi\ansicpg950\cocoartf1671
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red42\green44\blue46;\red255\green255\blue255;\red0\green0\blue0;
\red0\green0\blue0;\red65\green65\blue65;}
{\*\expandedcolortbl;;\cssrgb\c21569\c22745\c23529;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;
\csgray\c0\c0;\cssrgb\c32157\c32157\c32157;}
\paperw11900\paperh16840\margl1440\margr1440\vieww12720\viewh11540\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # Hierarchical Cluster\
	- Dendogram\
	- no need to specify number of clusters K\
	- no iterative refinement\
	- types:\
		* Agglomerative: bottom up\
			* ex: Agglomerative nesting (AGNES)\
					-> from single link (nearest neighbor)\
					-> sensitive to outlier\
					-> complete link: also sensitive to outlier\
					-> centroid link\
		* Divisive: top down\
			* inverse of AGNES\
			* recursive split\
			* consider as global search\
\
\
# Additional Method of Clusters (Hierarchical Clustering)\
	1. BIRCH: micro clustering based method\
		- \cf2 \cb3 \expnd0\expndtw0\kerning0
First partitioned hierarchically using a tree structure into microclusters\
			-> then clustered into macroclusters via other clustering algorithms\
\cf0 \cb1 \kerning1\expnd0\expndtw0 \
	2. CURE (Clustering using well-scattered representative):\
		- minimum distance between the representative points \
			-> (using both single link and average link)\
		- shrinking factor alpha: shrunk towards the centroid\
			-> more robust to outlier\
\
	3. CHAMELEON (Graph partitioning on the KNN)\
		Two phase algorithms\
		- Use graph partitioning algorithms\
		- Use agglomerative hierarchical clustering algorithms\
\
		Summary: \cf2 \cb3 \expnd0\expndtw0\kerning0
Once a k-nearest-neighbor graph is constructed, \
				it is partitioned into small graphlets, \
				which are then merged back together to create clusters.\
\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf2 \cb3 Problem of algorithmic hierarchical clustering:\
	- Nontrivial to choose a good distance measure\
	- Hard to handle missing attribute values\
	- Optimization goal not clear: local search\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf2 \cb3 \
\
\
# Probabilistic Hierarchical Clustering\
	- Use probabilistic models to measure distance between clusters\
	- Generative model -> ex: Gaussian model\
	- Can handle partial data\
\
	Summary:\
	1. When measuring the quality of a clustering, we take the product of the likelihood of each individual cluster.\cf2 \cb1 \
	2. \cf2 \cb3 Probabilistic hierarchical clustering is considered a generative model: the set of data objects to be clustered is assumed to be drawn from some underlying data generation mechanism.\
	3. \cf4 \cb5 \shad\shadx0\shady-20\shadr0\shado0 \shadc0 We only merge two clusters if their distance is < 0, which results in an increase in the clustering quality.\cf2 \cb3 \shad0 \
\
\
	\
\
\
\
\
\
\
\
\
}