{\rtf1\ansi\ansicpg950\cocoartf1671
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
# k-means\
	- apply only to continuous data\
	- sensitive to outlier -> k-median, k-mode to solve this issue\
	- limited to local optimal -> select initial points is very important or use k-means++\
	- cannot apply to non-convex clusters which mean can only apply for linear data \
		-> kernel k-means deal with non-linear\
\
# k-mode\
	- apply to categorical data\
\
# k-prototype\
	- use when there is mixed data (continuous and categorical data)\
\
# k-medoid\
	- solve the issue of k-means which is sensitive to outlier\
	- how it works:\
		- select initial K medoids randomly\
		- Object re-assign\
		- Swap medoid m with o if it improves the clustering quality\
		- repeat above two till it converge\
	- drawback: computational intentisive ( K*(n-K)^2)\
\
# kernel-kmeans\
	- for non-linear data\
	- computational intensive -> compute n*n kernel\
\
}